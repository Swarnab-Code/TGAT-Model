{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 1: Mount Google Drive & Load Dataset"
      ],
      "metadata": {
        "id": "zZFoZCOnqlvh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PfZc79j_piWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2360d90-726b-464a-9026-2f22b8909e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   Unnamed: 0.1                                        from  \\\n",
            "0      10159229  0x219c5355f7496c47e743f5a6d98527509ea42444   \n",
            "1      10010990  0x21a1662d90d163f79f9e71fda42c60926e80699c   \n",
            "2      10159517  0xcbe64fb9fdee1eb4172d2bc375c12ace497ac253   \n",
            "3      13323550  0x007077061537f25eaf485a1e6fa4af64e883be98   \n",
            "4      13323549  0x7a44dbe0d1823cd177a9b4c35899046190811fb3   \n",
            "\n",
            "                                           to  amount     timestamp  \\\n",
            "0  0xaaaf91d9b90df800df4f55c205fd6989c977e73a     0.0  1.494145e+09   \n",
            "1  0xaaaf91d9b90df800df4f55c205fd6989c977e73a     0.0  1.494196e+09   \n",
            "2  0xaaaf91d9b90df800df4f55c205fd6989c977e73a     0.0  1.494145e+09   \n",
            "3  0xf0f8b0b8dbb1124261fc8d778e2287e3fd2cf4f5     0.0  1.494185e+09   \n",
            "4  0xf0f8b0b8dbb1124261fc8d778e2287e3fd2cf4f5     0.0  1.494184e+09   \n",
            "\n",
            "   fromIsPhi  toIsPhi                 date  Unnamed: 0  \n",
            "0          0        0  2017-05-07 00:00:00         NaN  \n",
            "1          0        0  2017-05-07 00:00:00         NaN  \n",
            "2          0        0  2017-05-07 00:00:00         NaN  \n",
            "3          0        0  2017-05-07 00:00:00         NaN  \n",
            "4          0        0  2017-05-07 00:00:00         NaN  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 69486 entries, 0 to 69485\n",
            "Data columns (total 9 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Unnamed: 0.1  69486 non-null  int64  \n",
            " 1   from          69486 non-null  object \n",
            " 2   to            69486 non-null  object \n",
            " 3   amount        69486 non-null  float64\n",
            " 4   timestamp     69486 non-null  float64\n",
            " 5   fromIsPhi     69486 non-null  int64  \n",
            " 6   toIsPhi       69486 non-null  int64  \n",
            " 7   date          69486 non-null  object \n",
            " 8   Unnamed: 0    38221 non-null  float64\n",
            "dtypes: float64(3), int64(3), object(3)\n",
            "memory usage: 4.8+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset using pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset\n",
        "csv_path = '/content/drive/MyDrive/TGAT Model/clean_combined_dataset.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Show first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Show useful info about columns & data types\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 2: Preprocess Data & Split by Month"
      ],
      "metadata": {
        "id": "RCY1JK33qowM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "    # Drop unwanted index columns\n",
        "    df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'], inplace=True, errors='ignore')\n",
        "\n",
        "    # Convert 'date' string column to datetime format\n",
        "    df['date'] = pd.to_datetime(df['date'], format='mixed', errors='coerce')\n",
        "\n",
        "    # Drop rows with invalid dates\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "\n",
        "    # Sort by timestamp for time consistency\n",
        "    df.sort_values(by='timestamp', inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)  # Optional but clean\n",
        "\n",
        "    # Create 'month' column for splitting\n",
        "    df['month'] = df['date'].dt.to_period(\"M\")\n",
        "\n",
        "    # Split DataFrame into dictionary of monthly splits\n",
        "    graph_splits = dict(tuple(df.groupby('month')))\n",
        "\n",
        "    # Debug prints\n",
        "    print(\"‚úÖ Dataset Preprocessed\")\n",
        "    print(\"üìÜ Total months found:\", len(graph_splits))\n",
        "    print(f\"üóìÔ∏è Found {len(graph_splits)} months:\", list(graph_splits.keys()))\n",
        "    print(f\"üßæ Total transactions: {len(df)}\")\n",
        "\n",
        "    return graph_splits\n",
        "\n",
        "graph_splits = preprocess(df)"
      ],
      "metadata": {
        "id": "6qhM_Oeiqq-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04714de3-f333-4d2b-c944-dfe2fbf6e642"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset Preprocessed\n",
            "üìÜ Total months found: 23\n",
            "üóìÔ∏è Found 23 months: [Period('2016-11', 'M'), Period('2017-03', 'M'), Period('2017-05', 'M'), Period('2017-06', 'M'), Period('2017-07', 'M'), Period('2017-08', 'M'), Period('2017-09', 'M'), Period('2017-10', 'M'), Period('2017-11', 'M'), Period('2017-12', 'M'), Period('2018-01', 'M'), Period('2018-02', 'M'), Period('2018-03', 'M'), Period('2018-04', 'M'), Period('2018-05', 'M'), Period('2018-06', 'M'), Period('2018-07', 'M'), Period('2018-08', 'M'), Period('2018-09', 'M'), Period('2018-10', 'M'), Period('2018-11', 'M'), Period('2018-12', 'M'), Period('2019-01', 'M')]\n",
            "üßæ Total transactions: 69486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 3: Build Directed Temporal Graph"
      ],
      "metadata": {
        "id": "eWeXU18itmqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "def build_graph_from_month(df_month):\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    for _, row in df_month.iterrows():\n",
        "        # Add edge with attributes\n",
        "        G.add_edge(row['from'], row['to'], amount=row['amount'], timestamp=row['timestamp'])\n",
        "\n",
        "        for node in [row['from'], row['to']]:\n",
        "            G.nodes[node]['is_phishing'] = max(G.nodes[node].get('is_phishing', 0), row['fromIsPhi'] if node == row['from'] else row['toIsPhi'])\n",
        "            G.nodes[node]['timestamp'] = row['timestamp']\n",
        "\n",
        "    return G\n"
      ],
      "metadata": {
        "id": "tkhrU6tPtnwU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 4: Add 14 Node + 3 Edge Features (PDTGA Section 4.4)"
      ],
      "metadata": {
        "id": "WeSbxsm2tu29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_features(G):\n",
        "    for node in G.nodes():\n",
        "        # 1Ô∏è‚É£ Inbound and outbound edges\n",
        "        in_edges = list(G.in_edges(node))\n",
        "        out_edges = list(G.out_edges(node))\n",
        "\n",
        "        in_deg = len(in_edges)\n",
        "        out_deg = len(out_edges)\n",
        "\n",
        "        recv_amt = [G[u][v]['amount'] for u, v in in_edges]\n",
        "        sent_amt = [G[u][v]['amount'] for u, v in out_edges]\n",
        "\n",
        "        total_recv = sum(recv_amt)\n",
        "        total_sent = sum(sent_amt)\n",
        "\n",
        "        avg_recv = total_recv / len(recv_amt) if recv_amt else 0.0\n",
        "        avg_sent = total_sent / len(sent_amt) if sent_amt else 0.0\n",
        "\n",
        "        balance = total_recv - total_sent\n",
        "\n",
        "        # 2Ô∏è‚É£ Additional temporal edge features\n",
        "        all_edges = in_edges + out_edges\n",
        "        edge_amounts = [G[u][v]['amount'] for u, v in all_edges]\n",
        "        timestamps = [G[u][v]['timestamp'] for u, v in all_edges]\n",
        "\n",
        "        mean_amt = sum(edge_amounts) / len(edge_amounts) if edge_amounts else 0.0\n",
        "        min_ts = min(timestamps) if timestamps else 0.0\n",
        "        max_ts = max(timestamps) if timestamps else 0.0\n",
        "\n",
        "        # 3Ô∏è‚É£ Assemble all 17 features: 14 static + 3 temporal edge-based\n",
        "        G.nodes[node]['x'] = [\n",
        "            in_deg, out_deg,\n",
        "            len(recv_amt), len(sent_amt),\n",
        "            total_recv, total_sent,\n",
        "            avg_recv, avg_sent,\n",
        "            balance,\n",
        "            0, 0, 0, 0, 0,  # üß© Padding (features 10‚Äì14)\n",
        "            mean_amt, min_ts, max_ts  # üïí Temporal edge-derived (15‚Äì17)\n",
        "        ]\n",
        "\n",
        "        # Needed for TGAT's œÜ(t)\n",
        "        G.nodes[node]['timestamp'] = max_ts\n"
      ],
      "metadata": {
        "id": "LGWxeJSXtwAB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Info 1:\n",
        "\n",
        "So the training process should be:"
      ],
      "metadata": {
        "id": "P1tWbOB-iQKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Graph Data] ‚ûù Temporal Graph Attention Layer ‚ûù Node Embeddings ‚ûù MLP ‚ûù Phishing Classification\n"
      ],
      "metadata": {
        "id": "d-POPTp7iSgP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "0t4igZPq8ar2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d811ead8-9d71-4ae0-bfcc-a40f9953c619"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.7.14)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp->torch-geometric) (4.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 5: Convert NetworkX ‚ûù PyTorch Geometric Data Object"
      ],
      "metadata": {
        "id": "G7_ZDmNutzO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import from_networkx\n",
        "import torch\n",
        "\n",
        "def convert_to_pyg(G):\n",
        "    # Extract node features and timestamp\n",
        "    pyg_data = from_networkx(G, group_node_attrs=['x', 'timestamp'])\n",
        "    pyg_data.x = pyg_data.x.float()  # [N, 17]\n",
        "    pyg_data.x = (pyg_data.x - pyg_data.x.mean(dim=0)) / (pyg_data.x.std(dim=0) + 1e-5)   # Normalize Features\n",
        "\n",
        "    # Build edge_index and edge timestamps manually\n",
        "    node_list = list(G.nodes())\n",
        "    node_id_map = {node: i for i, node in enumerate(node_list)}\n",
        "\n",
        "    # Build edge_index manually\n",
        "    edge_index = []\n",
        "    edge_timestamps = []\n",
        "\n",
        "    for u, v, attr in G.edges(data=True):\n",
        "        edge_index.append([node_id_map[u], node_id_map[v]])\n",
        "        edge_timestamps.append(attr['timestamp'])\n",
        "\n",
        "    pyg_data.edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    pyg_data.edge_timestamp = torch.tensor(edge_timestamps, dtype=torch.float)\n",
        "\n",
        "    # Label tensor\n",
        "    pyg_data.y = torch.tensor(\n",
        "        [G.nodes[n]['is_phishing'] for n in node_list],\n",
        "        dtype=torch.long\n",
        "    )\n",
        "\n",
        "    # Timestamp tensor\n",
        "    pyg_data.timestamp = torch.tensor(\n",
        "        [G.nodes[n]['timestamp'] for n in node_list],\n",
        "        dtype=torch.float\n",
        "    )\n",
        "\n",
        "    return pyg_data\n"
      ],
      "metadata": {
        "id": "4nbnDf9d1Gqb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 6.1: Temporal Encoding œÜ(t) (Section 3.2)"
      ],
      "metadata": {
        "id": "pBRLwT3zuz04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def time_encoding(t_diff, d_model=16, max_time_scale=10000.0):\n",
        "    \"\"\"\n",
        "    Sinusoidal time encoding as described in PDTGA (TGAT-based).\n",
        "\n",
        "    Args:\n",
        "        t_diff (Tensor): Tensor of shape [num_edges] or [batch_size]\n",
        "        d_model (int): Dimension of the time embedding (should match TGAT time_dim)\n",
        "        max_time_scale (float): Scaling constant for frequency range\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Time embedding of shape [num_edges, d_model]\n",
        "    \"\"\"\n",
        "    # Ensure float32 for precision\n",
        "    t_diff = t_diff.float().unsqueeze(1)  # [E, 1]\n",
        "\n",
        "    # Generate frequency base (same as in transformers)\n",
        "    div_term = torch.exp(\n",
        "        torch.arange(0, d_model, 2, dtype=torch.float32) * (-np.log(max_time_scale) / d_model)\n",
        "    )  # shape: [d_model // 2]\n",
        "\n",
        "    # Compute sinusoidal encoding\n",
        "    sinusoid = torch.zeros((t_diff.shape[0], d_model), device=t_diff.device)\n",
        "    sinusoid[:, 0::2] = torch.sin(t_diff * div_term)  # even indices\n",
        "    sinusoid[:, 1::2] = torch.cos(t_diff * div_term)  # odd indices\n",
        "    return sinusoid  # [num_edges, d_model]\n"
      ],
      "metadata": {
        "id": "GU6a4fmHu0zU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 6.2: 2-Layer TGAT Module"
      ],
      "metadata": {
        "id": "LiAL7ipmu39Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TGATLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, time_dim=16):\n",
        "        super().__init__()\n",
        "        self.time_dim = time_dim\n",
        "        self.attn = GATConv(in_dim + time_dim, out_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, src_ts, dst_ts):\n",
        "        t_diff = dst_ts - src_ts  # [E]\n",
        "        t_enc = time_encoding(t_diff, d_model=self.time_dim)  # [E, time_dim]\n",
        "        x_src = x[edge_index[0]]  # Only source node features for each edge: [E, F]\n",
        "\n",
        "        # üü¶ Augment only the source node features\n",
        "        x_aug = torch.cat([x, torch.zeros(x.size(0), self.time_dim, device=x.device)], dim=1)\n",
        "        x_aug[edge_index[0]] = torch.cat([x_src, t_enc], dim=1)\n",
        "\n",
        "        return self.attn(x_aug, edge_index)\n",
        "\n",
        "class PDTGA(nn.Module):\n",
        "    def __init__(self, node_dim, time_dim=16, hidden_dim=50):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.layer1 = TGATLayer(node_dim, hidden_dim, time_dim)\n",
        "        self.layer2 = TGATLayer(hidden_dim, hidden_dim, time_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, timestamps):\n",
        "        src_ts = timestamps[edge_index[0]]\n",
        "        dst_ts = timestamps[edge_index[1]]\n",
        "        x = self.dropout(F.relu(self.layer1(x, edge_index, src_ts, dst_ts)))\n",
        "        x = self.dropout(F.relu(self.layer2(x, edge_index, src_ts, dst_ts)))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "LIxhpDxXu6VS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 7: MLP Classifier"
      ],
      "metadata": {
        "id": "0owEZGZGvBRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PhishingClassifier(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(32, 1),     # Only 1 output neuron for binary classification\n",
        "            nn.Sigmoid()          # Sigmoid activation\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)  # Output shape: [N, 1]\n"
      ],
      "metadata": {
        "id": "Ay3ShZR-vDue"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TGAT Model Pipeline (PDTGA)"
      ],
      "metadata": {
        "id": "2HboEmJE7kEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÅ CSV ‚ûù üßº Clean ‚ûù üåê Graph ‚ûù üßÆ Features ‚ûù üß† Time Encoding œÜ(t) ‚ûù üß≤ TGAT (2 layers) ‚ûù üéØ MLP ‚ûù üìä Evaluation\n"
      ],
      "metadata": {
        "id": "Z6U_hoc47mxo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 8: Train TGAT Model (BCELoss + Adam + Early Stop + Flood)"
      ],
      "metadata": {
        "id": "hGSHa98AvGI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# üß† Define model and classifier\n",
        "model = PDTGA(node_dim=18, time_dim=16, hidden_dim=50)  # 17 static + 1 timestamp = 18\n",
        "clf = PhishingClassifier(in_dim=50)      # input: output from TGAT, dim=50\n",
        "\n",
        "# üéØ Combine parameters\n",
        "params = list(model.parameters()) + list(clf.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=0.01)\n",
        "\n",
        "# üåä Flooding + Early Stop\n",
        "flood_level = 0.3\n",
        "early_stop = 10\n",
        "no_improve = 0\n",
        "best_f1 = 0.0\n",
        "tolerance = 0.001\n",
        "threshold = 0.7\n",
        "\n",
        "# üîÅ Training loop\n",
        "month_list = list(graph_splits.keys())\n",
        "train_months = month_list[:18]\n",
        "print(f\"üß™ Test months ({len(train_months)}): {train_months}\")\n",
        "\n",
        "for month in train_months:\n",
        "    print(f\"\\nüü¶ Training on: {month}\")\n",
        "    G = build_graph_from_month(graph_splits[month])\n",
        "    add_features(G)\n",
        "    data = convert_to_pyg(G)\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        clf.train()\n",
        "\n",
        "        out = model(data.x, data.edge_index, data.timestamp)\n",
        "        logits = clf(out).squeeze()  # Shape: [N]\n",
        "\n",
        "        # Prepare labels for BCELoss\n",
        "        targets = data.y.float()  # Shape: [N]\n",
        "\n",
        "        # üìå Apply weights manually\n",
        "        weights = torch.ones_like(targets)\n",
        "        weights[data.y == 1] = 5.0  # phishing = 1 ‚ûù weight 30.0\n",
        "\n",
        "        bce = nn.BCELoss(weight=weights)\n",
        "\n",
        "        # üí• Apply BCE + Flooding\n",
        "        loss_raw = bce(logits, targets)\n",
        "        # loss = (loss_raw - flood_level).abs() + flood_level  # flooding trick\n",
        "        loss = loss_raw\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # üß™ Monitor basic performance\n",
        "        with torch.no_grad():\n",
        "            pred = (logits >= threshold).long()  # Convert probabilities to class (0 or 1)\n",
        "            f1 = f1_score(data.y.cpu(), pred.cpu(), average='macro')\n",
        "            correct = (pred == data.y).sum().item()\n",
        "            acc = correct / len(data.y)\n",
        "\n",
        "        if f1 > best_f1 + tolerance:\n",
        "            best_f1 = f1\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve >= early_stop:\n",
        "            print(f\"‚õî Early stopping at epoch {epoch} due to no F1 improvement\")\n",
        "            break\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f\"Epoch {epoch:02d} | Loss: {loss.item():.4f} | Acc: {acc:.2%} | F1: {f1:.4f}\")\n",
        "\n",
        "    print(f\"üéØ Classifier on {month}: Loss {loss.item():.4f} | Acc {acc:.2%}\")\n"
      ],
      "metadata": {
        "id": "teoWxGisvG81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0809bb7f-9dcf-459f-ae43-130de052c629"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Test months (18): [Period('2016-11', 'M'), Period('2017-03', 'M'), Period('2017-05', 'M'), Period('2017-06', 'M'), Period('2017-07', 'M'), Period('2017-08', 'M'), Period('2017-09', 'M'), Period('2017-10', 'M'), Period('2017-11', 'M'), Period('2017-12', 'M'), Period('2018-01', 'M'), Period('2018-02', 'M'), Period('2018-03', 'M'), Period('2018-04', 'M'), Period('2018-05', 'M'), Period('2018-06', 'M'), Period('2018-07', 'M'), Period('2018-08', 'M')]\n",
            "\n",
            "üü¶ Training on: 2016-11\n",
            "Epoch 00 | Loss: 1.5250 | Acc: 66.67% | F1: 0.4000\n",
            "Epoch 05 | Loss: 1.1551 | Acc: 66.67% | F1: 0.4000\n",
            "Epoch 10 | Loss: 1.1117 | Acc: 100.00% | F1: 1.0000\n",
            "Epoch 15 | Loss: 0.3596 | Acc: 100.00% | F1: 1.0000\n",
            "‚õî Early stopping at epoch 17 due to no F1 improvement\n",
            "üéØ Classifier on 2016-11: Loss 0.0866 | Acc 100.00%\n",
            "\n",
            "üü¶ Training on: 2017-03\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2017-03: Loss 0.2235 | Acc 100.00%\n",
            "\n",
            "üü¶ Training on: 2017-05\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2017-05: Loss 0.7866 | Acc 88.77%\n",
            "\n",
            "üü¶ Training on: 2017-06\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2017-06: Loss 0.7526 | Acc 86.83%\n",
            "\n",
            "üü¶ Training on: 2017-07\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2017-07: Loss 1.1720 | Acc 91.64%\n",
            "\n",
            "üü¶ Training on: 2017-08\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2017-08: Loss 1.1702 | Acc 75.91%\n",
            "\n",
            "üü¶ Training on: 2017-09\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2017-09: Loss 0.8365 | Acc 90.17%\n",
            "\n",
            "üü¶ Training on: 2017-10\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2017-10: Loss 0.7708 | Acc 90.07%\n",
            "\n",
            "üü¶ Training on: 2017-11\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2017-11: Loss 0.5988 | Acc 94.25%\n",
            "\n",
            "üü¶ Training on: 2017-12\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2017-12: Loss 0.6764 | Acc 93.19%\n",
            "\n",
            "üü¶ Training on: 2018-01\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2018-01: Loss 0.6520 | Acc 93.94%\n",
            "\n",
            "üü¶ Training on: 2018-02\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2018-02: Loss 0.7748 | Acc 93.59%\n",
            "\n",
            "üü¶ Training on: 2018-03\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2018-03: Loss 0.7799 | Acc 95.50%\n",
            "\n",
            "üü¶ Training on: 2018-04\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2018-04: Loss 0.7741 | Acc 93.94%\n",
            "\n",
            "üü¶ Training on: 2018-05\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2018-05: Loss 1.0464 | Acc 89.96%\n",
            "\n",
            "üü¶ Training on: 2018-06\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2018-06: Loss 0.9629 | Acc 88.39%\n",
            "\n",
            "üü¶ Training on: 2018-07\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2018-07: Loss 0.9908 | Acc 86.99%\n",
            "\n",
            "üü¶ Training on: 2018-08\n",
            "‚õî Early stopping at epoch 0 due to no F1 improvement\n",
            "üéØ Classifier on 2018-08: Loss 0.6354 | Acc 91.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 9: Evaluate on Test Months"
      ],
      "metadata": {
        "id": "UHmwsQpWvJkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "# 1Ô∏è‚É£ Prepare storage\n",
        "all_true, all_pred, all_probs = [], [], []\n",
        "\n",
        "# ‚úÖ Define your months\n",
        "month_list = list(graph_splits.keys())\n",
        "test_months = month_list[18:]\n",
        "print(f\"üß™ Test months ({len(test_months)}): {test_months}\")\n",
        "\n",
        "# 2Ô∏è‚É£ Loop over test months\n",
        "for month in test_months:\n",
        "    print(f\"\\nüü• Testing on: {month}\")\n",
        "\n",
        "    # Build test graph\n",
        "    G = build_graph_from_month(graph_splits[month])\n",
        "    add_features(G)\n",
        "    data = convert_to_pyg(G)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        clf.eval()\n",
        "\n",
        "        out = model(data.x, data.edge_index, data.timestamp)\n",
        "        logits = clf(out).squeeze()  # [N]\n",
        "        pred = (logits >= 0.5).long()\n",
        "\n",
        "    # Store for summary\n",
        "    all_true.extend(data.y.cpu().numpy())\n",
        "    all_pred.extend(pred.cpu().numpy())\n",
        "    all_probs.extend(logits.cpu().numpy())\n",
        "\n",
        "# 3Ô∏è‚É£ Combined summary\n",
        "print(\"\\nTest Summary:\")\n",
        "\n",
        "all_true = np.array(all_true)\n",
        "all_pred = np.array(all_pred)\n",
        "all_probs = np.array(all_probs)\n",
        "\n",
        "print(\"Total nodes in test data:\", len(all_true))\n",
        "print(\"Total NORMAL nodes (true):\", np.sum(all_true == 0))\n",
        "print(\"Total PHISHING nodes (true):\", np.sum(all_true == 1))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(all_true, all_pred)\n",
        "print(\"\\nüî¢ Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(all_true, all_pred, target_names=[\"Normal\", \"Phishing\"], zero_division=0))\n",
        "\n",
        "# AUC Score\n",
        "try:\n",
        "    auc = roc_auc_score(all_true, all_probs)\n",
        "    print(f\"üìà AUC Score: {auc:.4f}\")\n",
        "except ValueError:\n",
        "    print(\"‚ö†Ô∏è AUC Score cannot be computed (only one class present in predictions).\")\n",
        "\n",
        "# Precision, Recall, F1 (macro)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(all_true, all_pred, average='macro', zero_division=0)\n",
        "print(f\"\\n‚öôÔ∏è Macro Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "# Detailed Metrics\n",
        "if confusion_matrix(all_true, all_pred).shape == (2, 2):\n",
        "    tn, fp, fn, tp = confusion_matrix(all_true, all_pred).ravel()\n",
        "    print(f\"\\n‚úÖ Normal correctly classified: {tn} / {tn + fp} = {tn / (tn + fp):.2%}\")\n",
        "    print(f\"‚úÖ Phishing correctly classified: {tp} / {tp + fn} = {tp / (tp + fn):.2%}\")\n"
      ],
      "metadata": {
        "id": "fpZDHWLEvOrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa12b8c-be69-4c98-e4e7-a18fe2e3a65b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Test months (5): [Period('2018-09', 'M'), Period('2018-10', 'M'), Period('2018-11', 'M'), Period('2018-12', 'M'), Period('2019-01', 'M')]\n",
            "\n",
            "üü• Testing on: 2018-09\n",
            "\n",
            "üü• Testing on: 2018-10\n",
            "\n",
            "üü• Testing on: 2018-11\n",
            "\n",
            "üü• Testing on: 2018-12\n",
            "\n",
            "üü• Testing on: 2019-01\n",
            "\n",
            "Test Summary:\n",
            "Total nodes in test data: 1003\n",
            "Total NORMAL nodes (true): 800\n",
            "Total PHISHING nodes (true): 203\n",
            "\n",
            "üî¢ Confusion Matrix:\n",
            "[[635 165]\n",
            " [ 50 153]]\n",
            "\n",
            "üìä Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.93      0.79      0.86       800\n",
            "    Phishing       0.48      0.75      0.59       203\n",
            "\n",
            "    accuracy                           0.79      1003\n",
            "   macro avg       0.70      0.77      0.72      1003\n",
            "weighted avg       0.84      0.79      0.80      1003\n",
            "\n",
            "üìà AUC Score: 0.7940\n",
            "\n",
            "‚öôÔ∏è Macro Precision: 0.7041 | Recall: 0.7737 | F1: 0.7213\n",
            "\n",
            "‚úÖ Normal correctly classified: 635 / 800 = 79.38%\n",
            "‚úÖ Phishing correctly classified: 153 / 203 = 75.37%\n"
          ]
        }
      ]
    }
  ]
}